import numpy as np
import torch
from torch import nn
import matplotlib.pyplot as plt


#å®šä¹‰ç­–ç•¥ç½‘ç»œ
class PolicyNet(nn.Module):
    def __init__(self):
        super(PolicyNet, self).__init__()
        self.fc1 = nn.Linear(1, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 2)
    # å‰å‘ä¼ æ’­
    def forward(self, x):  #
        x = torch.relu(self.fc1(x)) #
        x = torch.relu(self.fc2(x))  #
        return self.fc3(x) #è¾“å‡ºåŠ¨ä½œåˆ†æ•°


def parabola(x):
    return (x-1)**2 + 1

policy_net = PolicyNet()
optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.01)

def train_PolicyNet(epochs=100, steps_per_epoch=100,epsilon=0.1,min_loss=0.001, model_path=""):
    search_path = []

    for epoch in range(epochs):
        state = torch.tensor([[np.random.rand() * 2]],dtype=torch.float32)  # åˆå§‹åŒ–çŠ¶
        #state = torch.tensor(5,dtype=torch.float32)
        optimizer.zero_grad() #æ¢¯åº¦æ¸…é›¶
        log_probs =[] # å­˜å‚¨logæ¦‚ç‡
        rewards =[] #
        for _ in range(steps_per_epoch):
            scores = policy_net(state)  # ä½œå–åŠ¨ä½œçš„åˆ†æ•°
            probabilities = torch.nn.functional.softmax(scores, dim = 1)  # è®¡ç®—åŠ¨ä½œæ¦‚ç‡
            m = torch.distributions.Categorical(probabilities)
            #action = torch.argmax(probabilities).item()

            #action = m.sample()  # ç±³æ ·ä¸€ä¸ªå±±ä½œ

            # ä½¿ç”¨ Îµ-greedy é€‰æ‹©åŠ¨ä½œ
            if np.random.rand() < epsilon:
                action = torch.randint(0, probabilities.shape[1], (1,)).item()  # 10% é€‰æ‹©éšæœºåŠ¨ä½œ
            else:
                action = torch.argmax(probabilities).item()  # 90% é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ

            #print(f"Action: {action}")

            #print(action)
            # æ ¹æ®çµæ ·çš„åŠ¨ä½œæ›´æ–°çŠ¶ç‚¹
            step_size = 0.01
            new_state = state - step_size if action == 0 else state + step_size
            reward =  ((parabola(state) - parabola(new_state))*200)**3
            # print(new_state)
            # print(reward)
            #reward =  1/ if parabola(new_state)-1 >= 2 else reward == -1# è®¡ç®—å¥–M
            #if parabola(new_state)-1 >= 2:
            #    reward = 1/(parabola(new_state)-1)
            #else:
            #    reward = -1

            #rewards.append(reward)
            rewards.append(torch.tensor([reward], dtype=torch.float32))  # é€‚é… PyTorch
            log_probs.append(m.log_prob(torch.tensor(action)))  # è®¡ç®— log æ¦‚ç‡
            state = new_state  # æ›´æ–°çŠ¶æ€
            search_path.append(state.item())

        rewards = torch.cat(rewards)  # æ‹¼æ¥å¥–åŠ±
        log_probs = torch.cat(log_probs)  # æ‹¼æ¥æ¦‚ç‡  è’™ç‰¹å¡æ´›
        loss = -torch.sum(log_probs * rewards)  # è®¡ç®—æŸå¤±
        loss.backward()  # åå‘ä¼ æ’­
        optimizer.step()  # æ›´æ–°å‚æ•°
        # ğŸ”¹ å¦‚æœ loss å°äº min_lossï¼Œæå‰åœæ­¢è®­ç»ƒ
        if abs(loss.item()) < min_loss:
            print(f"Early stopping at epoch {epoch + 1}, Loss: {loss.item()}")
            break  # é€€å‡ºè®­ç»ƒå¾ªç¯
        #
        #     log_prob = m.log_prob(action)  # iè·¯LogM
        #     log_probs.append(log_prob)
        #     state = new_state  # æ–°çŠ¶
        #     search_path.append(state.item())
        #
        # rewards = torch.cat(rewards) #æ‹¼æ¥å¥–åŠ±
        # log_probs = torch.cat(log_probs) #æ‹¼æ¥æ¦‚ç‡
        # loss = -torch.sum(torch.mul(log_probs,rewards))  # è®¡ç®—æŸå¤±
        # loss.backward()  # æ±‰å‘ä¼ 
        # optimizer.step() # ç«æ–°å‚æ¬¢
        # **ä¿å­˜æ¨¡å‹**
        #torch.save(policy_net.state_dict(), model_path)
        #print(f"Model saved to {model_path}")
        if (epoch + 1)% 100 == 0:
            print(f"Epoch {epoch +1}/{epochs}, Loss: {loss.item()}")

    # **ä¿å­˜æ¨¡å‹**
    torch.save(policy_net.state_dict(), model_path)
    print(f"Model saved to {model_path}")

    return search_path

#train_PolicyNet(100, 100)


def test_policy(policy_net, steps=100):
    state = torch.tensor([[np.random.rand() * 2]], dtype=torch.float32)  # éšæœºåˆå§‹åŒ–çŠ¶æ€
    search_path = [state.item()]  # è®°å½•æœç´¢è·¯å¾„

    for _ in range(steps):
        scores = policy_net(state)  # è·å–ç­–ç•¥ç½‘ç»œçš„åˆ†æ•°
        probabilities = torch.nn.functional.softmax(scores, dim=1)  # è®¡ç®—åŠ¨ä½œæ¦‚ç‡
        action = torch.argmax(probabilities).item()  # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åŠ¨ä½œ

        step_size = 0.01
        new_state = state - step_size if action == 0 else state + step_size  # æ›´æ–°çŠ¶æ€
        state = new_state  # èµ‹å€¼æ–°çš„çŠ¶æ€
        search_path.append(state.item())  # è®°å½•è·¯å¾„

    return search_path



import matplotlib.pyplot as plt

def plot_search_path(search_path):
    plt.figure(figsize=(8, 5))
    plt.plot(search_path, marker="o", linestyle="-", color="b", alpha=0.7, label="Search Path")
    plt.xlabel("Step")
    plt.ylabel("State")
    plt.title("Policy Search Path")
    plt.legend()
    plt.grid(True)
    plt.show()


# è®­ç»ƒæ¨¡å‹
train_PolicyNet(epochs=10000, steps_per_epoch=100, epsilon=0.1, min_loss=0.0001, model_path="C:/Users/28123/Desktop/policy_net.pth")

# æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
#search_path = test_policy(policy_net, steps=1000)

# å¯è§†åŒ–æœç´¢è·¯å¾„
#plot_search_path(search_path)

#
# def load_model(model_path="policy_net.pth"):
#     model = PolicyNet()
#     model.load_state_dict(torch.load(model_path))
#     model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
#     print(f"Model loaded from {model_path}")
#     return model
#
# trained_model = load_model("C:/Users/28123/Desktop/policy_net.pth")
# search_path = test_policy(trained_model, steps=100)
# plot_search_path(search_path)
